{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNGZyz43ZKjZR9i2H9EMWei"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Namesformer\n"],"metadata":{"id":"kZZa-Eh_xVXl"}},{"cell_type":"markdown","source":["Streamlit application with already trained models can be accessed here: [Namesformer Streamlit app](https://namesformer-project-n75w6hw3x87mgi4mjbfaat.streamlit.app/). Also you can access [Git repository](https://github.com/MariusGvergzdys/Namesformer-project/tree/main) to see the same file and other additional files. To construct Lithuanian name generator firstly we import necessary modules."],"metadata":{"id":"OUCfbN4_xeP7"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"rWPafHMQ2yFw","executionInfo":{"status":"ok","timestamp":1733145514415,"user_tz":-120,"elapsed":4197,"user":{"displayName":"Marius Gvergždys","userId":"00415568837627424984"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"markdown","source":["Secondly, we acccess all Lithuanian male and female names, leaving out special characters to make an easier model training process. After executing this, two text files are saved each containing information about male and female names (with special characters excluded)."],"metadata":{"id":"XuZAo_ay18BE"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","\n","names_v = []\n","names_m = []\n","\n","for key in ['a', 'b', 'c', 'c-2', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n","            'm', 'n', 'o', 'p', 'r', 's', 's-2', 't', 'u', 'v', 'z', 'z-2']:\n","    url = f'https://vardai.vlkk.lt/sarasas/{key}/'\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","    links1 = soup.find_all('a', class_='names_list__links names_list__links--man')\n","    links2 = soup.find_all('a', class_='names_list__links names_list__links--woman')\n","    names_v += [name.text for name in links1]\n","    names_m += [name.text for name in links2]\n","\n","# Mapping dictionary to convert special characters to simpler forms\n","char_map = {\n","    'ò': 'o', 'ỹ': 'y', 'È': 'E', 'ù': 'u', 'ĩ': 'i',\n","    'ý': 'y', 'è': 'e', '̀': '', '́': '', '̃': '', 'é': 'e',\n","    'Ù': 'U', 'Ẽ': 'E', 'ò': 'o', 'ì': 'i', 'õ': 'o', 'á': 'a',\n","    'Õ': 'O', 'ẽ': 'e', 'ã': 'a', 'Ã': 'A', 'ũ': 'u', 'ó': 'o', 'ñ': 'n',\n","    'Ì': 'I', 'à': 'a', 'Ò': 'O', 'ỹ': 'y', 'Á': 'A'\n","}\n","\n","# Function to replace special characters using the mapping\n","def normalize_name(name, char_map):\n","    return ''.join(char_map.get(char, char) for char in name)\n","\n","# Apply the normalization to the list of names\n","normalized_names_v = [normalize_name(name, char_map) for name in names_v]\n","normalized_names_m = [normalize_name(name, char_map) for name in names_m]\n","\n","np.savetxt('vardai_v.txt', normalized_names_v, fmt='%s', header='name', comments='', newline='\\n')\n","np.savetxt('vardai_m.txt', normalized_names_m, fmt='%s', header='name', comments='', newline='\\n')"],"metadata":{"id":"wa5pgB78Om0C","executionInfo":{"status":"ok","timestamp":1733145786624,"user_tz":-120,"elapsed":28263,"user":{"displayName":"Marius Gvergždys","userId":"00415568837627424984"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["We add a space at the end to mark the end of the name. This applies for both male and female, because two datasets will be constructed."],"metadata":{"id":"JUtM4aJ46tON"}},{"cell_type":"code","source":["class NameDataset(Dataset):\n","    def __init__(self, csv_file):\n","        self.names = pd.read_csv(csv_file)['name'].values\n","        self.chars = sorted(list(set(''.join(self.names) + ' ')))  # Including a padding character\n","        self.char_to_int = {c: i for i, c in enumerate(self.chars)}\n","        self.int_to_char = {i: c for c, i in self.char_to_int.items()}\n","        self.vocab_size = len(self.chars)\n","\n","    def __len__(self):\n","        return len(self.names)\n","\n","    def __getitem__(self, idx):\n","          name = self.names[idx] + ' '  # Adding padding character at the end\n","          encoded_name = [self.char_to_int[char] for char in name]\n","          return torch.tensor(encoded_name)\n"],"metadata":{"id":"yGmQPzMSOrRx","executionInfo":{"status":"ok","timestamp":1733145814274,"user_tz":-120,"elapsed":3,"user":{"displayName":"Marius Gvergždys","userId":"00415568837627424984"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["dataset_v = NameDataset('vardai_v.txt')\n","dataset_m = NameDataset('vardai_m.txt')\n","print ('Size of male dataset:',len(names_v))\n","print('Size of female dataset:',len(names_m))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VfZksFFtOuX9","executionInfo":{"status":"ok","timestamp":1733145816389,"user_tz":-120,"elapsed":5,"user":{"displayName":"Marius Gvergždys","userId":"00415568837627424984"}},"outputId":"73814137-d200-4ef4-9cd1-28d501e3aa66"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of male dataset: 3850\n","Size of female dataset: 4235\n"]}]},{"cell_type":"markdown","source":["Encoded name for the first male name."],"metadata":{"id":"Vj41URnM7jWB"}},{"cell_type":"code","source":["dataset_v[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IGh9tVvJOuxf","executionInfo":{"status":"ok","timestamp":1733145823241,"user_tz":-120,"elapsed":1777,"user":{"displayName":"Marius Gvergždys","userId":"00415568837627424984"}},"outputId":"790e3a55-d5da-4144-fe12-bbaf05b6d3a1"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 1, 24, 23, 40,  0])"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["Decoded name for the first male name."],"metadata":{"id":"ZYEMY_FK7n-_"}},{"cell_type":"code","source":["[dataset_v.int_to_char[int(char)] for char in dataset_v[0]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pZaaD2UHOwLi","executionInfo":{"status":"ok","timestamp":1733145827719,"user_tz":-120,"elapsed":4,"user":{"displayName":"Marius Gvergždys","userId":"00415568837627424984"}},"outputId":"933fbd25-446c-46cc-f88c-671099d92b51"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['A', 'b', 'a', 's', ' ']"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["Here we define the way to construct padded batches."],"metadata":{"id":"Am9MfrKt8Ojz"}},{"cell_type":"code","source":["# Custom collate function for padding\n","def pad_collate(batch):\n","    padded_seqs = pad_sequence(batch, batch_first=True, padding_value=0)\n","    input_seq = padded_seqs[:, :-1]\n","    target_seq = padded_seqs[:, 1:]\n","    return input_seq, target_seq\n","\n","dataloader_v = DataLoader(dataset_v, batch_size=32, shuffle=True, collate_fn=pad_collate)\n","dataloader_m = DataLoader(dataset_m, batch_size=32, shuffle=True, collate_fn=pad_collate)"],"metadata":{"id":"z-udGa3kOxqZ","executionInfo":{"status":"ok","timestamp":1733145881120,"user_tz":-120,"elapsed":1032,"user":{"displayName":"Marius Gvergždys","userId":"00415568837627424984"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["next(iter(dataloader_v))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EgzjSKVKOz-G","executionInfo":{"status":"ok","timestamp":1733078871444,"user_tz":-120,"elapsed":444,"user":{"displayName":"Marius Gvergždys","userId":"00415568837627424984"}},"outputId":"38f16476-95d5-45b2-c90d-7cc926826b82"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 1, 39, 41, 30, 42, 39,  0,  0,  0,  0,  0,  0],\n","         [11, 23, 81, 39, 38, 23, 40,  0,  0,  0,  0,  0],\n","         [ 5, 36, 82, 39, 31, 33, 23, 40,  0,  0,  0,  0],\n","         [13, 31, 80, 34, 26, 23, 39, 23, 40,  0,  0,  0],\n","         [ 1, 36, 41, 37, 80, 36, 23, 40,  0,  0,  0,  0],\n","         [21, 31, 80, 36, 29, 31, 36, 41, 23, 40,  0,  0],\n","         [ 5, 35, 23, 36, 42, 27, 80, 34, 31, 40,  0,  0],\n","         [ 7, 27, 26, 23, 35, 31, 36, 23, 40,  0,  0,  0],\n","         [10, 37, 81, 43, 31, 36, 23, 40,  0,  0,  0,  0],\n","         [13, 31, 33, 37, 34, 37, 32, 42, 40,  0,  0,  0],\n","         [16, 27, 39, 82, 34, 31, 40,  0,  0,  0,  0,  0],\n","         [16, 23, 34, 27, 35, 37, 80, 36, 23, 40,  0,  0],\n","         [ 2, 27, 36,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","         [ 4, 23, 36, 31, 27, 34, 40,  0,  0,  0,  0,  0],\n","         [ 1, 39, 40, 27, 36, 31, 32, 40,  0,  0,  0,  0],\n","         [ 1, 34, 82, 35, 27, 26, 23, 40,  0,  0,  0,  0],\n","         [ 5, 31, 82, 35, 23, 40,  0,  0,  0,  0,  0,  0],\n","         [78, 43, 23, 31, 29, 79, 26, 46, 81, 36, 23, 40],\n","         [ 5, 31, 82, 43, 23, 40,  0,  0,  0,  0,  0,  0],\n","         [ 1, 39, 40, 70, 36, 31, 32, 42, 40,  0,  0,  0],\n","         [ 5, 80, 35, 31, 40,  0,  0,  0,  0,  0,  0,  0],\n","         [ 1, 35, 23, 26, 70, 82, 32, 42, 40,  0,  0,  0],\n","         [15, 80, 40, 43, 31, 36, 23, 40,  0,  0,  0,  0],\n","         [16, 23, 24, 31, 32, 37, 36, 23, 40,  0,  0,  0],\n","         [16, 23, 40, 25, 23, 34,  0,  0,  0,  0,  0,  0],\n","         [ 7, 27, 39, 30, 23, 81, 39, 26, 23, 40,  0,  0],\n","         [10, 42, 80, 34, 31, 40,  0,  0,  0,  0,  0,  0],\n","         [21, 46, 81, 34, 31, 23, 42, 26, 23, 40,  0,  0],\n","         [ 1, 36, 26, 37, 81, 32, 23, 40,  0,  0,  0,  0],\n","         [21, 31, 34, 41, 83, 36, 31, 40,  0,  0,  0,  0],\n","         [ 2, 39, 37, 36, 31, 80, 40, 34, 37, 43, 23, 40],\n","         [ 7, 23, 42, 26, 83, 36, 31, 40,  0,  0,  0,  0]]),\n"," tensor([[39, 41, 30, 42, 39,  0,  0,  0,  0,  0,  0,  0],\n","         [23, 81, 39, 38, 23, 40,  0,  0,  0,  0,  0,  0],\n","         [36, 82, 39, 31, 33, 23, 40,  0,  0,  0,  0,  0],\n","         [31, 80, 34, 26, 23, 39, 23, 40,  0,  0,  0,  0],\n","         [36, 41, 37, 80, 36, 23, 40,  0,  0,  0,  0,  0],\n","         [31, 80, 36, 29, 31, 36, 41, 23, 40,  0,  0,  0],\n","         [35, 23, 36, 42, 27, 80, 34, 31, 40,  0,  0,  0],\n","         [27, 26, 23, 35, 31, 36, 23, 40,  0,  0,  0,  0],\n","         [37, 81, 43, 31, 36, 23, 40,  0,  0,  0,  0,  0],\n","         [31, 33, 37, 34, 37, 32, 42, 40,  0,  0,  0,  0],\n","         [27, 39, 82, 34, 31, 40,  0,  0,  0,  0,  0,  0],\n","         [23, 34, 27, 35, 37, 80, 36, 23, 40,  0,  0,  0],\n","         [27, 36,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","         [23, 36, 31, 27, 34, 40,  0,  0,  0,  0,  0,  0],\n","         [39, 40, 27, 36, 31, 32, 40,  0,  0,  0,  0,  0],\n","         [34, 82, 35, 27, 26, 23, 40,  0,  0,  0,  0,  0],\n","         [31, 82, 35, 23, 40,  0,  0,  0,  0,  0,  0,  0],\n","         [43, 23, 31, 29, 79, 26, 46, 81, 36, 23, 40,  0],\n","         [31, 82, 43, 23, 40,  0,  0,  0,  0,  0,  0,  0],\n","         [39, 40, 70, 36, 31, 32, 42, 40,  0,  0,  0,  0],\n","         [80, 35, 31, 40,  0,  0,  0,  0,  0,  0,  0,  0],\n","         [35, 23, 26, 70, 82, 32, 42, 40,  0,  0,  0,  0],\n","         [80, 40, 43, 31, 36, 23, 40,  0,  0,  0,  0,  0],\n","         [23, 24, 31, 32, 37, 36, 23, 40,  0,  0,  0,  0],\n","         [23, 40, 25, 23, 34,  0,  0,  0,  0,  0,  0,  0],\n","         [27, 39, 30, 23, 81, 39, 26, 23, 40,  0,  0,  0],\n","         [42, 80, 34, 31, 40,  0,  0,  0,  0,  0,  0,  0],\n","         [46, 81, 34, 31, 23, 42, 26, 23, 40,  0,  0,  0],\n","         [36, 26, 37, 81, 32, 23, 40,  0,  0,  0,  0,  0],\n","         [31, 34, 41, 83, 36, 31, 40,  0,  0,  0,  0,  0],\n","         [39, 37, 36, 31, 80, 40, 34, 37, 43, 23, 40,  0],\n","         [23, 42, 26, 83, 36, 31, 40,  0,  0,  0,  0,  0]]))"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","source":["We need to define model architecture and do the training. Two models are constructed - one trained on male dataset and other trained on female dataset."],"metadata":{"id":"QXOJrT5i8ZKa"}},{"cell_type":"code","source":["class MinimalTransformer(nn.Module):\n","    def __init__(self, vocab_size, embed_size, num_heads, forward_expansion):\n","        super(MinimalTransformer, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))\n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)\n","        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n","        self.output_layer = nn.Linear(embed_size, vocab_size)\n","\n","    def forward(self, x):\n","        positions = torch.arange(0, x.size(1)).unsqueeze(0)\n","        x = self.embed(x) + self.positional_encoding[:, :x.size(1), :]\n","        x = self.transformer_encoder(x)\n","        x = self.output_layer(x)\n","        return x\n","\n","# Training Loop\n","def train_model(model, dataloader, epochs=150):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters())\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0.0\n","        batch_count = 0\n","\n","        for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n","            optimizer.zero_grad()\n","\n","            output = model(input_seq)\n","            loss = criterion(output.transpose(1, 2), target_seq)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            batch_count += 1\n","\n","        average_loss = total_loss / batch_count\n","        print(f'Epoch {epoch+1}, Average Loss: {average_loss}')\n","\n","\n","model_v = MinimalTransformer(vocab_size=dataset_v.vocab_size, embed_size=128, num_heads=8, forward_expansion=4)\n","model_m = MinimalTransformer(vocab_size=dataset_m.vocab_size, embed_size=128, num_heads=8, forward_expansion=4)\n","\n","print(\"Training of male model on male dataset:\")\n","train_model(model_v, dataloader_v)\n","print(\"Training female model on female dataset:\")\n","train_model(model_m, dataloader_m)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vc2LJbeMO1mC","executionInfo":{"status":"ok","timestamp":1733146493872,"user_tz":-120,"elapsed":438010,"user":{"displayName":"Marius Gvergždys","userId":"00415568837627424984"}},"outputId":"eaa186bb-af25-4e21-ddec-574933f09676"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training of male model on male dataset:\n","Epoch 1, Average Loss: 1.3734787200108047\n","Epoch 2, Average Loss: 1.180070554421953\n","Epoch 3, Average Loss: 1.1540861523841037\n","Epoch 4, Average Loss: 1.1403056851103286\n","Epoch 5, Average Loss: 1.144808336230349\n","Epoch 6, Average Loss: 1.134744022503372\n","Epoch 7, Average Loss: 1.1300937277226408\n","Epoch 8, Average Loss: 1.1303028370723252\n","Epoch 9, Average Loss: 1.1253380849341716\n","Epoch 10, Average Loss: 1.1246882578558173\n","Epoch 11, Average Loss: 1.125990235115871\n","Epoch 12, Average Loss: 1.1195545713763593\n","Epoch 13, Average Loss: 1.1168948737057773\n","Epoch 14, Average Loss: 1.123084168296215\n","Epoch 15, Average Loss: 1.1194742819494452\n","Epoch 16, Average Loss: 1.1131768344847623\n","Epoch 17, Average Loss: 1.1064387673188831\n","Epoch 18, Average Loss: 1.1134696982123635\n","Epoch 19, Average Loss: 1.105308040607074\n","Epoch 20, Average Loss: 1.1157481985643876\n","Epoch 21, Average Loss: 1.111461024639035\n","Epoch 22, Average Loss: 1.1101557341488926\n","Epoch 23, Average Loss: 1.1073487711346839\n","Epoch 24, Average Loss: 1.1041668811120278\n","Epoch 25, Average Loss: 1.1067314172579237\n","Epoch 26, Average Loss: 1.107915815243051\n","Epoch 27, Average Loss: 1.1017537117004395\n","Epoch 28, Average Loss: 1.10501780884325\n","Epoch 29, Average Loss: 1.0999744209376248\n","Epoch 30, Average Loss: 1.1089142856519085\n","Epoch 31, Average Loss: 1.0968616486580904\n","Epoch 32, Average Loss: 1.104480749812008\n","Epoch 33, Average Loss: 1.0966897528033612\n","Epoch 34, Average Loss: 1.102990927775044\n","Epoch 35, Average Loss: 1.1009737381265183\n","Epoch 36, Average Loss: 1.09785099019689\n","Epoch 37, Average Loss: 1.1049503900788047\n","Epoch 38, Average Loss: 1.0971318094198368\n","Epoch 39, Average Loss: 1.103223629234251\n","Epoch 40, Average Loss: 1.088738582350991\n","Epoch 41, Average Loss: 1.093481043153558\n","Epoch 42, Average Loss: 1.0992074786138928\n","Epoch 43, Average Loss: 1.0942696884643932\n","Epoch 44, Average Loss: 1.1018218432576203\n","Epoch 45, Average Loss: 1.0968588418211818\n","Epoch 46, Average Loss: 1.0986723402314935\n","Epoch 47, Average Loss: 1.0957741402397472\n","Epoch 48, Average Loss: 1.1017396612600847\n","Epoch 49, Average Loss: 1.096853583804832\n","Epoch 50, Average Loss: 1.10067784293624\n","Epoch 51, Average Loss: 1.100459825894064\n","Epoch 52, Average Loss: 1.1070502986592694\n","Epoch 53, Average Loss: 1.0967850320595356\n","Epoch 54, Average Loss: 1.0917530049962445\n","Epoch 55, Average Loss: 1.0980432604955248\n","Epoch 56, Average Loss: 1.0945385093531332\n","Epoch 57, Average Loss: 1.099816988814961\n","Epoch 58, Average Loss: 1.091426091253265\n","Epoch 59, Average Loss: 1.091016148240113\n","Epoch 60, Average Loss: 1.10061571617757\n","Epoch 61, Average Loss: 1.0865942697879696\n","Epoch 62, Average Loss: 1.0976656591596683\n","Epoch 63, Average Loss: 1.092391374682592\n","Epoch 64, Average Loss: 1.0884394409242741\n","Epoch 65, Average Loss: 1.1006143506893442\n","Epoch 66, Average Loss: 1.0919636285994663\n","Epoch 67, Average Loss: 1.0935581651600925\n","Epoch 68, Average Loss: 1.0909940639803233\n","Epoch 69, Average Loss: 1.0928754727702497\n","Epoch 70, Average Loss: 1.0897223245013843\n","Epoch 71, Average Loss: 1.0885937130155643\n","Epoch 72, Average Loss: 1.0946048774009893\n","Epoch 73, Average Loss: 1.093518847768957\n","Epoch 74, Average Loss: 1.0956301960078152\n","Epoch 75, Average Loss: 1.0850487622347744\n","Epoch 76, Average Loss: 1.09402584980342\n","Epoch 77, Average Loss: 1.0841374958842254\n","Epoch 78, Average Loss: 1.0998240906344958\n","Epoch 79, Average Loss: 1.0908648731294743\n","Epoch 80, Average Loss: 1.0946958084736973\n","Epoch 81, Average Loss: 1.093525258962773\n","Epoch 82, Average Loss: 1.08533561476006\n","Epoch 83, Average Loss: 1.093681311804401\n","Epoch 84, Average Loss: 1.0859758592834157\n","Epoch 85, Average Loss: 1.090188966802329\n","Epoch 86, Average Loss: 1.0953576712568929\n","Epoch 87, Average Loss: 1.0842861296716801\n","Epoch 88, Average Loss: 1.0933110201654355\n","Epoch 89, Average Loss: 1.0873624303124168\n","Epoch 90, Average Loss: 1.0946337447678747\n","Epoch 91, Average Loss: 1.0901189663193442\n","Epoch 92, Average Loss: 1.086724239932604\n","Epoch 93, Average Loss: 1.088393011368996\n","Epoch 94, Average Loss: 1.0944023083064183\n","Epoch 95, Average Loss: 1.0892525078836552\n","Epoch 96, Average Loss: 1.0885070532806649\n","Epoch 97, Average Loss: 1.0883808490658595\n","Epoch 98, Average Loss: 1.0813292092528224\n","Epoch 99, Average Loss: 1.0859815153208645\n","Epoch 100, Average Loss: 1.0831294458759717\n","Epoch 101, Average Loss: 1.0872922455968936\n","Epoch 102, Average Loss: 1.0884753938548821\n","Epoch 103, Average Loss: 1.0823849419916958\n","Epoch 104, Average Loss: 1.0816025206865358\n","Epoch 105, Average Loss: 1.0903045106525264\n","Epoch 106, Average Loss: 1.0867423794486306\n","Epoch 107, Average Loss: 1.0902080053140308\n","Epoch 108, Average Loss: 1.0903484698169488\n","Epoch 109, Average Loss: 1.0876420923501007\n","Epoch 110, Average Loss: 1.0846157359682824\n","Epoch 111, Average Loss: 1.0945633713864098\n","Epoch 112, Average Loss: 1.083467080573405\n","Epoch 113, Average Loss: 1.0851365815509448\n","Epoch 114, Average Loss: 1.0925034806748066\n","Epoch 115, Average Loss: 1.0842988328500227\n","Epoch 116, Average Loss: 1.083257263356989\n","Epoch 117, Average Loss: 1.0837047577889498\n","Epoch 118, Average Loss: 1.08603382504676\n","Epoch 119, Average Loss: 1.0935756933590597\n","Epoch 120, Average Loss: 1.0829261541366577\n","Epoch 121, Average Loss: 1.0844587831457784\n","Epoch 122, Average Loss: 1.0874224448007\n","Epoch 123, Average Loss: 1.0811353380029851\n","Epoch 124, Average Loss: 1.0864145982363993\n","Epoch 125, Average Loss: 1.1003694864344005\n","Epoch 126, Average Loss: 1.0882248981925082\n","Epoch 127, Average Loss: 1.090247995597272\n","Epoch 128, Average Loss: 1.0903595543104756\n","Epoch 129, Average Loss: 1.0878737036846886\n","Epoch 130, Average Loss: 1.0820383922127652\n","Epoch 131, Average Loss: 1.0863648692438426\n","Epoch 132, Average Loss: 1.0809021360618023\n","Epoch 133, Average Loss: 1.0930520325652824\n","Epoch 134, Average Loss: 1.0802942159747289\n","Epoch 135, Average Loss: 1.0897883516697844\n","Epoch 136, Average Loss: 1.0872447736992323\n","Epoch 137, Average Loss: 1.0799138344023838\n","Epoch 138, Average Loss: 1.0870736812757067\n","Epoch 139, Average Loss: 1.087392249383217\n","Epoch 140, Average Loss: 1.0847305409179246\n","Epoch 141, Average Loss: 1.083896696074935\n","Epoch 142, Average Loss: 1.0871810839195881\n","Epoch 143, Average Loss: 1.0842143952353926\n","Epoch 144, Average Loss: 1.0890623153733814\n","Epoch 145, Average Loss: 1.0884458654183002\n","Epoch 146, Average Loss: 1.0852500387459747\n","Epoch 147, Average Loss: 1.077493285344652\n","Epoch 148, Average Loss: 1.0884170231740338\n","Epoch 149, Average Loss: 1.0873799545705811\n","Epoch 150, Average Loss: 1.0911756936183645\n","Training female model on female dataset:\n","Epoch 1, Average Loss: 1.4063621705636047\n","Epoch 2, Average Loss: 1.2268273283664446\n","Epoch 3, Average Loss: 1.2027944554959922\n","Epoch 4, Average Loss: 1.1858715930379422\n","Epoch 5, Average Loss: 1.179936991598373\n","Epoch 6, Average Loss: 1.1816725161738861\n","Epoch 7, Average Loss: 1.1734759274281954\n","Epoch 8, Average Loss: 1.1713084134840428\n","Epoch 9, Average Loss: 1.1756603897066045\n","Epoch 10, Average Loss: 1.1685467765743571\n","Epoch 11, Average Loss: 1.1720962201742302\n","Epoch 12, Average Loss: 1.1616149038300478\n","Epoch 13, Average Loss: 1.1653485988315784\n","Epoch 14, Average Loss: 1.162029166418807\n","Epoch 15, Average Loss: 1.1567496289884238\n","Epoch 16, Average Loss: 1.1567159613272302\n","Epoch 17, Average Loss: 1.1583044811298973\n","Epoch 18, Average Loss: 1.1562816980189847\n","Epoch 19, Average Loss: 1.1548191031118982\n","Epoch 20, Average Loss: 1.1588137615892224\n","Epoch 21, Average Loss: 1.149974523182202\n","Epoch 22, Average Loss: 1.1569922759120626\n","Epoch 23, Average Loss: 1.1631251860381966\n","Epoch 24, Average Loss: 1.1470187687336053\n","Epoch 25, Average Loss: 1.1518008032239468\n","Epoch 26, Average Loss: 1.1501572190370775\n","Epoch 27, Average Loss: 1.151548601630935\n","Epoch 28, Average Loss: 1.1507472109077568\n","Epoch 29, Average Loss: 1.1416849946617185\n","Epoch 30, Average Loss: 1.1461274959987267\n","Epoch 31, Average Loss: 1.1424793829595237\n","Epoch 32, Average Loss: 1.138960778265071\n","Epoch 33, Average Loss: 1.1521877780892795\n","Epoch 34, Average Loss: 1.1423752648489816\n","Epoch 35, Average Loss: 1.1436551679345899\n","Epoch 36, Average Loss: 1.151504493297491\n","Epoch 37, Average Loss: 1.145157816266655\n","Epoch 38, Average Loss: 1.1482100379198117\n","Epoch 39, Average Loss: 1.156296183740286\n","Epoch 40, Average Loss: 1.1484513896748536\n","Epoch 41, Average Loss: 1.1485002412831873\n","Epoch 42, Average Loss: 1.1566800152448784\n","Epoch 43, Average Loss: 1.1516094637992687\n","Epoch 44, Average Loss: 1.1407580178483088\n","Epoch 45, Average Loss: 1.1459055453314817\n","Epoch 46, Average Loss: 1.1485057845151514\n","Epoch 47, Average Loss: 1.1476570632224692\n","Epoch 48, Average Loss: 1.1456044352144228\n","Epoch 49, Average Loss: 1.1528573094454027\n","Epoch 50, Average Loss: 1.1445839983180053\n","Epoch 51, Average Loss: 1.14189687931448\n","Epoch 52, Average Loss: 1.1441840412921476\n","Epoch 53, Average Loss: 1.1366239270769565\n","Epoch 54, Average Loss: 1.14232085253063\n","Epoch 55, Average Loss: 1.147853868348258\n","Epoch 56, Average Loss: 1.1352518550435404\n","Epoch 57, Average Loss: 1.1444724121488126\n","Epoch 58, Average Loss: 1.143598555174089\n","Epoch 59, Average Loss: 1.1397495964415987\n","Epoch 60, Average Loss: 1.1447595474415255\n","Epoch 61, Average Loss: 1.1427790379165708\n","Epoch 62, Average Loss: 1.1428744797419785\n","Epoch 63, Average Loss: 1.1474822894074863\n","Epoch 64, Average Loss: 1.1363237043072407\n","Epoch 65, Average Loss: 1.1375151706817455\n","Epoch 66, Average Loss: 1.1342572188915168\n","Epoch 67, Average Loss: 1.1338452871580769\n","Epoch 68, Average Loss: 1.137196291658215\n","Epoch 69, Average Loss: 1.1386786865112477\n","Epoch 70, Average Loss: 1.1390684475576072\n","Epoch 71, Average Loss: 1.143962888341201\n","Epoch 72, Average Loss: 1.136926229286911\n","Epoch 73, Average Loss: 1.143191263191682\n","Epoch 74, Average Loss: 1.1349602579174185\n","Epoch 75, Average Loss: 1.1448597822870528\n","Epoch 76, Average Loss: 1.139480468025781\n","Epoch 77, Average Loss: 1.135875207589085\n","Epoch 78, Average Loss: 1.1381725830243046\n","Epoch 79, Average Loss: 1.1360221218345756\n","Epoch 80, Average Loss: 1.1371680408492124\n","Epoch 81, Average Loss: 1.1409637573966407\n","Epoch 82, Average Loss: 1.13463228447993\n","Epoch 83, Average Loss: 1.1346448534413387\n","Epoch 84, Average Loss: 1.1394043685798358\n","Epoch 85, Average Loss: 1.1327899985743644\n","Epoch 86, Average Loss: 1.1336491045198942\n","Epoch 87, Average Loss: 1.1349608588935738\n","Epoch 88, Average Loss: 1.1414164779777813\n","Epoch 89, Average Loss: 1.1393507257440036\n","Epoch 90, Average Loss: 1.1397688532234134\n","Epoch 91, Average Loss: 1.1415927531127643\n","Epoch 92, Average Loss: 1.1371194370707174\n","Epoch 93, Average Loss: 1.143852934801489\n","Epoch 94, Average Loss: 1.1417078550596882\n","Epoch 95, Average Loss: 1.1348194179678321\n","Epoch 96, Average Loss: 1.1268162709429748\n","Epoch 97, Average Loss: 1.139171478443576\n","Epoch 98, Average Loss: 1.1351340690949805\n","Epoch 99, Average Loss: 1.1386932354224355\n","Epoch 100, Average Loss: 1.1381928934190506\n","Epoch 101, Average Loss: 1.1361790487640782\n","Epoch 102, Average Loss: 1.1343281650005426\n","Epoch 103, Average Loss: 1.1387374692393424\n","Epoch 104, Average Loss: 1.1318035596295406\n","Epoch 105, Average Loss: 1.1399244719878174\n","Epoch 106, Average Loss: 1.131836214011773\n","Epoch 107, Average Loss: 1.136814874813969\n","Epoch 108, Average Loss: 1.134790689873516\n","Epoch 109, Average Loss: 1.1409043678663726\n","Epoch 110, Average Loss: 1.1348074096486085\n","Epoch 111, Average Loss: 1.1434796842417323\n","Epoch 112, Average Loss: 1.1324115023577124\n","Epoch 113, Average Loss: 1.135819861763402\n","Epoch 114, Average Loss: 1.1301495751043908\n","Epoch 115, Average Loss: 1.1310621733055974\n","Epoch 116, Average Loss: 1.1290579281355206\n","Epoch 117, Average Loss: 1.1369147659244394\n","Epoch 118, Average Loss: 1.1353683682312643\n","Epoch 119, Average Loss: 1.1345104575157166\n","Epoch 120, Average Loss: 1.1429379515181808\n","Epoch 121, Average Loss: 1.138192271827755\n","Epoch 122, Average Loss: 1.1301453265928685\n","Epoch 123, Average Loss: 1.1325431994925765\n","Epoch 124, Average Loss: 1.1351257728454762\n","Epoch 125, Average Loss: 1.135113547619124\n","Epoch 126, Average Loss: 1.13508105009122\n","Epoch 127, Average Loss: 1.1395059599912256\n","Epoch 128, Average Loss: 1.1297527677134465\n","Epoch 129, Average Loss: 1.1382365051965069\n","Epoch 130, Average Loss: 1.12869936272614\n","Epoch 131, Average Loss: 1.1414881317239058\n","Epoch 132, Average Loss: 1.1329596468380518\n","Epoch 133, Average Loss: 1.1281855953367133\n","Epoch 134, Average Loss: 1.142470465566879\n","Epoch 135, Average Loss: 1.1315006878142966\n","Epoch 136, Average Loss: 1.13118090575799\n","Epoch 137, Average Loss: 1.1373671257406248\n","Epoch 138, Average Loss: 1.132560866667812\n","Epoch 139, Average Loss: 1.1234415563425624\n","Epoch 140, Average Loss: 1.130551697168135\n","Epoch 141, Average Loss: 1.1283016886029924\n","Epoch 142, Average Loss: 1.1323565527012474\n","Epoch 143, Average Loss: 1.1238961627608852\n","Epoch 144, Average Loss: 1.137108586336437\n","Epoch 145, Average Loss: 1.1316529736483008\n","Epoch 146, Average Loss: 1.135018521681764\n","Epoch 147, Average Loss: 1.1318606563976832\n","Epoch 148, Average Loss: 1.1303718291727223\n","Epoch 149, Average Loss: 1.1351682458605086\n","Epoch 150, Average Loss: 1.129642146870606\n"]}]},{"cell_type":"markdown","source":["Lastly, sample function is constructed to extract generated name."],"metadata":{"id":"OMcA-SXc8zUW"}},{"cell_type":"code","source":["def sample(model, dataset, start_str='a', max_length=20, temperature=1.0):\n","    assert temperature > 0, \"Temperature must be greater than 0\"\n","    model.eval()\n","    with torch.no_grad():\n","        # Convert start string to tensor\n","        chars = [dataset.char_to_int[c] for c in start_str]\n","        input_seq = torch.tensor(chars).unsqueeze(0)  # Add batch dimension\n","\n","        output_name = start_str\n","        for _ in range(max_length - len(start_str)):\n","            output = model(input_seq)\n","\n","            # Apply temperature scaling\n","            logits = output[0, -1] / temperature\n","            probabilities = torch.softmax(logits, dim=0)\n","\n","            # Sample a character from the probability distribution\n","            next_char_idx = torch.multinomial(probabilities, 1).item()\n","            next_char = dataset.int_to_char[next_char_idx]\n","\n","            if next_char == ' ':\n","                break\n","\n","            # Add next character to the generated name\n","            output_name += next_char\n","\n","            # Update the input sequence for the next iteration\n","            input_seq = torch.cat([input_seq, torch.tensor([[next_char_idx]])], dim=1)\n","\n","        return output_name\n","\n","\n","print('Sampling male names:')\n","for _ in range(10):\n","    generated_name =  sample(model_v, dataset_v,  start_str='R', temperature=0.7)\n","    print(' ',generated_name)\n","\n","print('Sampling female names:')\n","for _ in range(10):\n","  generated_name = sample(model_m, dataset_m, start_str = 'R', temperature = 0.7)\n","  print(' ',generated_name)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36Rr0-h1TMV2","executionInfo":{"status":"ok","timestamp":1733146581274,"user_tz":-120,"elapsed":644,"user":{"displayName":"Marius Gvergždys","userId":"00415568837627424984"}},"outputId":"1c6c94ca-81ab-4771-fdd7-bbd3301626ea"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampling male names:\n","  Rarnantijus\n","  Rinantas\n","  Radmas\n","  Rygimas\n","  Ronis\n","  Raugis\n","  Ririaudas\n","  Ranas\n","  Romantas\n","  Rongis\n","Sampling female names:\n","  Rairycija\n","  Rarita\n","  Rarinė\n","  Rilitija\n","  Rinenana\n","  Raimintė\n","  Rara\n","  Raurga\n","  Ranija\n","  Rertė\n"]}]},{"cell_type":"code","source":["torch.save(model_v, 'namesformer_model_v.pth')\n","torch.save(model_m, 'namesformer_model_m.pth')"],"metadata":{"id":"L_AaBfaJVldz","executionInfo":{"status":"ok","timestamp":1733146608918,"user_tz":-120,"elapsed":1098,"user":{"displayName":"Marius Gvergždys","userId":"00415568837627424984"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# Conclusions"],"metadata":{"id":"3QJ0iViw9LZR"}},{"cell_type":"markdown","source":["The whole idea was to create two datasets each corresponding to male and female names and create two models trained on different datasets. That might seem pretty inefficient. However, different approach was also tested - to create one dataset of male and female names. End markers were used as follows: '%' to mark the end of the male name and '#' to mark the end of the female name. Later, one model was trained on the whole dataset. Also, sample function was a bit modified adding parameter 'sex' to indicate which gender names should be generated. Nevertheless, model generating, for instance, 10 male names on average generated 2-3 female names out of 10 and vice versa. It seemed that model faces problems on distinguishing gender perfectly. Higher number of epochs in training and number of layers did not give significantly better results. This approach would  be more efficient, but I did not manage to make it more accurate on distinguishing gender, thus I chose the first approach, which seems to be more inefficient, but considerably more accurate."],"metadata":{"id":"6JkeYH9h9Zug"}}]}